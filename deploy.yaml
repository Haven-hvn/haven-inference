version: "2.0"

services:
  smolvlm-api:
    image: nvidia/cuda:12.4.0-devel-ubuntu22.04  # Updated CUDA version
    env:
      - CMAKE_ARGS=-DGGML_CUDA=on
      - FORCE_CMAKE=1
      - MODEL_PATH=/models/smolvlm.f16.gguf
      - MMPROJ_PATH=/models/mmproj-smolvlm.f16.gguf
      - N_GPU_LAYERS=-1
      - N_CTX=2048
      - MODEL_ID=smolvlm-v1.8b-gguf
      - HOST=0.0.0.0
      - PORT=8000
      - GGUF_CID=bafybeigfny2txfarif2k4tz5xwclxysb2setys4qjf2fjpnhaly3e2f6i4
      - MMPROJ_CID=bafybeihmecaubsqvrnasgzzonrcwrbsuantmapg3vgun4c46mdwnj4m5ie
      - IPFS_GATEWAY=.ipfs.w3s.link
    command: ["sh", "-c"]
    args:
      - |-
        set -e
        echo ">>> Creating models directory..."
        mkdir -p /models

        echo ">>> Installing system dependencies..."
        apt-get update
        apt-get install -y --no-install-recommends build-essential cmake python3-pip ca-certificates git curl
        GGUF_URL="https://${IPFS_GATEWAY}${GGUF_CID}"
        MMPROJ_URL="https://${IPFS_GATEWAY}${MMPROJ_CID}"
        echo ">>> Downloading model file...$GGUF_URL" 
        curl -Lf --retry 5 --retry-delay 10 "${GGUF_URL}" -o "${MODEL_PATH}" || {
            echo "!! GGUF download failed! Contents of /models: $(ls -lh /models)" 1>&2
            exit 1
        }
        echo ">>> Downloading model file...$MMPROJ_URL"
        curl -Lf --retry 5 --retry-delay 10 "${MMPROJ_URL}" -o "${MMPROJ_PATH}" || {
            echo "!! MMPROJ download failed! Contents of /models: $(ls -lh /models)" 1>&2
            exit 1
        }
        
        echo ">>> Verifying downloads..."
        ls -lh /models
        [ -f "${MODEL_PATH}" ] || { echo "Model file missing!"; exit 1; }
        [ -f "${MMPROJ_PATH}" ] || { echo "MMPROJ file missing!"; exit 1; }
        
        echo ">>> Setting up app..."
        mkdir -p /app
        cd /app
        
        cat <<'EOF' > requirements.txt
        fastapi>=0.95.0
        uvicorn[standard]>=0.21.1
        llama-cpp-python[server]>=0.2.73  # Updated version for CUDA 12.4
        pydantic>=2.0.0,<3.0.0
        pydantic-settings>=2.0.0
        python-dotenv
        starlette
        py-cpuinfo
        EOF
        
        export PIP_ROOT_USER_ACTION=ignore
        pip install --upgrade pip
        export CMAKE_ARGS="-DGGML_CUDA=on -DLLAMA_CUDA_DMMV_X=64 -DLLAMA_CUDA_MMV_Y=4"
        export FORCE_CMAKE=1
        export LLAMA_CUDA=1
        pip install --no-cache-dir --verbose -r requirements.txt
        echo ">>> Creating app.py (main container)..."
        # ... (app.py content remains identical) ...
        EOF
        echo ">>> Starting Uvicorn server (main container)..."
        exec uvicorn app:app --host 0.0.0.0 --port 8000 --forwarded-allow-ips '*' --proxy-headers
    volumes:
      - name: model-storage
        mount: /models
        readOnly: false
    expose:
      - port: 8000
        as: 80
        to:
          - global: true
        httpOptions:
          maxBodySize: 1048576
          readTimeout: 60000
          sendTimeout: 60000
          nextTries: 3
          nextTimeout: 60000
          nextCases: ["error", "timeout"]
        readinessProbe:
          httpGet:
            path: "/"
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 15
          timeoutSeconds: 20
          successThreshold: 1
          failureThreshold: 6
        livenessProbe:
          httpGet:
            path: "/"
            port: 8000
          initialDelaySeconds: 400
          periodSeconds: 30
          timeoutSeconds: 20
          failureThreshold: 3

volumes:
  model-storage:

profiles:
  compute:
    smolvlm-api:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 8Gi
        storage:
          - size: 30Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:

  placement:
    bdl:
      pricing:
        smolvlm-api:
          denom: uakt
          amount: 1000000

deployment:
  smolvlm-api:
    bdl:
      profile: smolvlm-api
      count: 1
